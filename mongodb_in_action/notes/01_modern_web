-----------------------------------------------------------------------
|  CHAPTER 1 - A DATABASE FOR THE MODERN WEB                          |
-----------------------------------------------------------------------

- Relational DBs

    - Well-normalized data model (data isn't duplicated)
    - Transactions
    - Durable storage engine



- Document DBs

    - Simple data model
    - High read and write throughput
    - Ability to scale easily with automatic failover



- Example of Data Simplicity

    Here is a simple user record:

      {  _id: 10,  
         username: 'peter',  
         email: 'pbbakkum@gmail.com'
      }

    Now, let's say you need multiple email addresses for the user.

      {  _id: 10,  
         username: 'peter',  
         email: [
           'pbbakkum@gmail.com',    
           'pbb7c@virginia.edu'
        ]
      }

    That's it.  No need to worry about fitting into a schema or adding more tables.



- MongoDB

    - Began in mid-2007 as a PaaS project by start-up company 10gen in 2007.


    - Later, the company changed it's name to 'MongoDB' and continues to sponsor the
        database's development as an open source project.  Most the project's core
        developers still work there.


    - MongoDB's data model is document-oriented.  The documents are JSON.


    - A document is essentially a set of property names and their values.

        // A post on a social news website (ie Reddit or Twitter)

        {   
            _id: ObjectID('4bd9e8e17cefd644108961bb'),     // Primary key
            title: 'Adventures in Databases',  
            url: 'http://example.com/databases.txt',  
            author: 'msmith',  
            vote_count: 20,  
            tags: ['databases', 'mongodb', 'indexing'],
            image: {                                       // Attribute pointing to another document
              url: 'http://example.com/db.jpg',
              caption: 'A database.',
              type: 'jpg',
              size: 75381,    
              data: 'Binary'
            },
            comments: [
              {
                user: 'bjones',
                text: 'Interesting article.'
              },
              { 
                user: 'sverch',
                text: 'Color me skeptical!'
              }
            ]
        }


    - Internally, MongoDB stores documents in a format called BSON (Binary JSON).  BSON
        has a similar structure but is used for storing many documents.


    - Instead of tables, MongoDB has 'Collections' of documents.  They are stored on disk.
        Most queries require specifying the collection we want to target.


    - There are usually a lot fewer collections than there would be tables in a relational
        DB, so a lot fewer joins are required.


    - Collections do not enforce any sort of schema.  Your application code enforces the 
        structure of the data, which can speed up initial development when the schema is
        changing frequently.  Also, the items can have a dynamic set of attributes.



- Ad Hoc Queries

    - Relational DBs support ad hoc queries, but some data stores do not.  For instance, 
        key/value stores are queryable by the key only.  Key/value store sacrifice rich
        query power in exchange for a simple scalability model.


    - One of MongoDB's design goals is to preserve most of the query power from relational
        DBs.


    - Suppose you want to find all the posts tagged 'politics' that have more than 10 votes.
        In SQL, the query would look like:

        SELECT * FROM posts
        INNER JOIN posts_tags ON posts.id = posts_tags.post_id
        INNER JOIN tags ON posts_tags.tag_id == tags.id 
        WHERE tags.text = 'politics' 
        AND posts.vote_count > 10;

      In Mongo, we would make this query using a document as a matcher.

        db.posts.find({'tags': 'politics', 'vote_count': {'$gt': 10}});



- Indexes

    - In order to avoid linear searches through collections we need to create indexes.
        Mongo uses B-Tree indexes.


    - Starting with Mongo 3.2, WiredTiger will also support LSM indexes.


    - Many NoSQL databases, such as HBase, are considered key/value stores, because they
        don't allow secondary indexes.  Mongo does allow secondary indexes, (ie can index
        by id and tag).


    - Mongo collections can have up to 64 indexes.  They include:

        - ascending
        - descending
        - unique
        - compound-key
        - hashed
        - text
        - geospatial



- Replication

    - Replica sets distribute data across 2 or more machines for redundancy and automate
        failover in the event of server failures or network outages.


    - Replication is also used to scale database reads.  Many web applications are 
        read-intensive.


    - Replica sets consist of many Mongo servers.  At any given time, one node is the primary
        and one or more nodes serve as secondaries.  The primary accepts reads and writes, 
        but the secondaries only accept reads.


    - If the primary node fails, the cluster will pick a secondary node and automatically 
        promote it to primary.  When the former primary comes back online, it will do so
        as a secondary.



- Speed and Durability

    - In database systems, there is an inverse relationship between write speed and
        durability.


    - For instance, if you write 100 records of 50 KB each to a database, then immediately
        cut power to the database, will those records be recoverable when you bring the
        machine back online?


    - Most databases enable good durability by default.  For some applications, like storing
        log lines, it might make sense to have faster data writes, even if you risk
        data loss.


    - The problems arise from the mismatches between writing to a hard drive and writing to
        RAM.  Certain databases, like Memcached, write only to RAM, which makes them 
        extremely fast but completely volatile.


    - On the other had, few databases write exclusively to disk, since this makes them
        extremely slow.  Database designers must find the best balance of speed and durability.


    - Transaction Logging

        - One compromise between speed and durability can be seen in MySQL's InnoDB.  Since it
            is a transactional storage engine, it must guarantee durability.  

        - It accomplishes this by writing its updates in 2 places:

            1. A transaction log
            2. An in-memory buffer pool

        - The transaction log is synced to disk immediately, whereas the buffer pool is only
            eventually synced by a background thread.

        - The reason for this dual write is that random I/O is much slower than sequential I/O.
            Writing to the transaction log is sequential, and the writes to the buffer pool are
            random.  

        - In the event of an unclean shutdown, the transaction log can be replayed to make sure
            all the updates succeed.



- Speed and Durability in Mongo

    - In Mongo's case, the users control the speed and durability trade-off by choosing write
        semantics and deciding whether to enable journaling.


    - Mongo guarantees that a write has been written to RAM before returning to the user, 
        though this characteristic is configurable.

        - You can configure Mongo to be 'fire-and-forget', sending off a write to the server
            without waiting for an acknowledgment.  Before v2.0, this was the default.

        - You can configure Mongo to guarantee that a write has gone to multiple replices before
            considering it committed.


    - Since v2.0, journaling has been enabled by default.  With 'journaling', every write is
        flushed to the journal file every 100 ms.  If an unclean shutdown happens, the journal
        will be used to ensure Mongo's data files are consistent when you restart the server.
        This is the safest way to run Mongo.


    - It's also possible to run Mongo without journaling to increase write performance.  The
        downside is that data files could be corrupted after an unclean shutdown, so anyone
        disabling journaling should run with replication, preferable to a second datacenter.



- Scaling

    - The easiest way to scale most databases is vertically scaling (aka 'scaling up')
        by upgrading the hardware.


    - When you reach the point where it's no longer feasible to move to a better machine,
        you want to scale horizontally (aka 'scaling out').  By distributing the data
        over multiple machines, you can reduce your hosting costs and mitigate the 
        consequences of failure.


    - Mongo uses a range-based partitioning mechanism called 'sharding', which automatically
        manages the distribution of data across nodes.  There are also hash- and tag-
        based sharding, but they're just another form of range-based sharding.


    - The sharding system handles the addition of shard nodes, and also facilitates an 
        automatic failover.  Individual shards are made up of a replica set containing at
        least 2 nodes, ensuring automatic recovery with no single point of failure.


    - Your application code doesn't need to handle any of these logistics.  It communicates
        with a sharded cluster just as it speaks to a single node.



- The Mongo Core Server

    - Mongo is written in C++ and runs on OSX, Windows, Solaris, and most flavors of Linux.


    - The core database server runs via an executable called 'mongod'.  The data files on 
        most Unix-like systems are stored in /data/db.


    - mongod can be run in several modes, such as a standalone server or a member of a replica
        set.  Replication is recommended in production.  You generally see replica set
        configurations consisting of 2 replicas and one mongod in arbiter mode.


    - When you use the sharding features, you will also run mongod in config server mode.
        Also, a separate server called 'mongos' is used to send requests to the appropriate
        instance in this kind of setup.



- The JavaScript Shell

    - The MongoDB command shell is a JavaScript-based tool for administering the DB and
        and manipulating data.



- Database Drivers

    - All drivers have functionality to query, retrieve results, write data, and run 
        database commands.


    - The representation of the document itself will usually be whatever is most natural 
        to each language. In Ruby, that means using a Ruby hash. In Python, a dictionary 
        is appropriate.


    - As of v3.0, there are drivers available for:

        C, C++, C#, Erlang, Java, Node.js, JavaScript, Perl, PHP, Python, Scala, and Ruby



- Command-Line Tools

    - 'mongodump' and 'mongorestore'

        These are utilities for backing up and restoring a database.  'mongodump' saves the
          database in its native BSON only.


    - 'mongoexport' and 'mongoimport'

        Export and import json, csv, and tsv data.


    - 'mongosniff'

        A wire-sniffing tool for viewing operations sent to the database.  It translates the
          BSON to human-readable shell statements.


    - 'mongostat'

        Similar to 'iostat', constantly polls MongoDB and the system for performance
          and resource utilization information.


    - 'mongotop'

        Similar to 'top', polls Mongo for the time spend reading and writing to each
          collection.


    - 'mongoperf'

        Helps you understand the disk operations happening in a running MongoDB instance.


    - 'mongooplog'

        Shows what's happening in the MongoDB oplog.


    - 'Bsondump'

        Converts BSON files into human-readable formats like JSON.



- Why Mongo?

    - Designed to combine the best features of key-values and relational databases.

      Key-Value stores are extremely fast and relatively easy to scale.  Relational 
        databases are more difficult to scale, but have a rich data model and a
        powerful query language.


    - Mongo is well-suited for:

        - a primary datastore for web applications
        - analytics and logging applications
        - any application requiring a medium-grade cache


    - Mongo is also good for capturing data whose structure can't be known in advance.



- Mongo vs Simple Key-Value Stores

    - Simple key-value stores like are often used for caching.  For instance, cache a web page 
        as {'url': 'html_page'}.  


    - Simple key-value stores store the value as an opaque byte array.  There is no concept of
        schema or data types.  You insert a value by key and can retrieve or delete it.
        Systems with this simplicity are fast and scalable.


    - The best-known is Memcached, which stores its data in memory only.  It trades durability
        for speed.  Memcached nodes running across multiple servers can act as a single datastore, 
        eliminating the complexity of maintaining cache state across machines.


    - Compared with Mongo, Memcached has faster reads and writes.  These systems cannot be used as
        primary datastores.  They are best used as caching layers atop traditional DBs.



- Mongo vs Sophisticated Key-Value Stores

    - It’s possible to refine the simple key-value model to handle complicated read/write 
        schemes or to provide a richer data model. In these cases, you end up with what we’ll
        term a sophisticated key-value store.


    - One example is Amazon Dynamo, which is designed to be a database robust enough to continue 
        functioning in the face of network failures, datacenter outages, and similar disruptions.


    - This requires that the system always be read from and written to, which essentially 
        requires that data be automatically replicated across multiple nodes. If a node fails, a 
        user of the system (ie a customer with a shopping cart) won’t experience any 
        interruptions in service.


    - Dynamo provides ways of resolving the inevitable conflicts that arise when a system allows 
        the same data to be written to multiple nodes. At the same time, Dynamo is easily scaled. 
        Because it’s masterless — all nodes are equal — it’s easy to understand the system as a 
        whole, and nodes can be added easily.  


    - Although Dynamo is a proprietary system, the ideas used to build it have inspired many 
        systems falling under the NoSQL umbrella, including Cassandra, HBase, and Riak KV.


    - By looking at who developed these sophisticated key-value stores, and how they’ve been used
        in practice, you can see where these systems shine.  Let’s  take  Cassandra, which 
        implements many of Dynamo’s scaling properties while providing a column-oriented data model
        inspired by Google’s BigTable.  

      Cassandra is an open source version of a datastore built by Facebook for its inbox search 
        feature.  The  system scales horizontally to index more than 50 TB of inbox data, allowing 
        for searches on inbox keywords and recipients.  Data is indexed by user ID, where each record 
        consists of an array of search terms for keyword searches and an array of recipient IDs for
        recipient searches.


    - These sophisticated key-value stores were developed by major internet companies, such as Amazon,
        Google, and Facebook to manage cross-sections of systems with extraordinarily large amounts  
        of data.


    - Because of their masterless architecture, these systems scale easily with the addition of nodes.
        They opt for eventual consistency, which means that reads don’t necessarily reflect the latest 
        write.  But what users get in exchange for weaker consistency is the ability to write in the 
        face of any one node’s failure.


    - MongoDB, on the other hand, provides strong consistency, a rich data model, and secondary
        indexes.  You can fetch records by primary key.



- Mongo vs Relational DBs

    - SQL can be more expressive and easier to work with than Mongo's query language.  Also,
        SQL is (mostly) portable between platforms.


    - Some SQL databases are intended for analytics (or 'data warehousing') rather than as an
        application database.  Usually data is imported in bulk and then queried by analysts
        in these platforms.  Teradata Database is dominant in this space, and they offer
        horizontally scalable databases.


    - Apache Hive allows for running SQL queries over data stored in Hadoop.  Hive translates
        a SQL query into a MapReduce job, which offers a scalable way of querying large
        data sets.  These queries are intended for slow analytics queries, not to be used in 
        an application.



- Mongo vs CouchDB

    - CouchDB's document model is similar, but data is stored in plain text JSON, as opposed
        to Mongo's BSON.


    - Secondary indexes in CouchDB are defined by writing map-reduce functions.


    - CouchDB doesn't partition data across nodes.  Each CouchDB nodes is a complete replica
        of every other.



- Mongo for Web Applications

    - Can run most of the same queries you can in a relational model.


    - Can be useful for powering a high-traffic website.


    - Many web applications use a caching layer to deliver content faster.  Since MongoDB is
        faster, switching from a relational DB to Mongo can often allow you to remove the
        Memcached layer altogether.



- Mongo for Analytics and Logging

    - Mongo's relavance to analytics derives from 2 key features:

        1. Targeted atomic updates

             Clients can efficiently increment counters and push values onto arrays.

        2. Capped collections

            Useful for logging, since they store only the most recent documents.
              Also, storing logs in the database rather than the file system allows for
              easier querying.



- Tips and Limitations

    - As with database servers in general, Mongo is best run on a dedicated server.


    - Since Mongo uses JSON, including storing the key of every entry, it does not use
        space particularly efficiently.